{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> MLflow with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import optuna\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import mlflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: review the links mentioned above for guidance on connecting to a managed tracking server, such as the free Databricks Community Edition\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate our synthetic training data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_apple_sales_data_with_promo_adjustment(\n",
    "    base_demand: int = 1000,\n",
    "    n_rows: int = 5000,\n",
    "    competitor_price_effect: float = -50.0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates a synthetic dataset for predicting apple sales demand with multiple\n",
    "    influencing factors.\n",
    "\n",
    "    This function creates a pandas DataFrame with features relevant to apple sales.\n",
    "    The features include date, average_temperature, rainfall, weekend flag, holiday flag,\n",
    "    promotional flag, price_per_kg, competitor's price, marketing intensity, stock availability,\n",
    "    and the previous day's demand. The target variable, 'demand', is generated based on a\n",
    "    combination of these features with some added noise.\n",
    "\n",
    "    Args:\n",
    "        base_demand (int, optional): Base demand for apples. Defaults to 1000.\n",
    "        n_rows (int, optional): Number of rows (days) of data to generate. Defaults to 5000.\n",
    "        competitor_price_effect (float, optional): Effect of competitor's price being lower\n",
    "                                                   on our sales. Defaults to -50.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with features and target variable for apple sales prediction.\n",
    "\n",
    "    Example:\n",
    "        >>> df = generate_apple_sales_data_with_promo_adjustment(base_demand=1200, n_rows=6000)\n",
    "        >>> df.head()\n",
    "    \"\"\"\n",
    "\n",
    "    # Set seed for reproducibility\n",
    "    np.random.seed(9999)\n",
    "\n",
    "    # Create date range\n",
    "    dates = [datetime.now() - timedelta(days=i) for i in range(n_rows)]\n",
    "    dates.reverse()\n",
    "\n",
    "    # Generate features\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"date\": dates,\n",
    "            \"average_temperature\": np.random.uniform(10, 35, n_rows),\n",
    "            \"rainfall\": np.random.exponential(5, n_rows),\n",
    "            \"weekend\": [(date.weekday() >= 5) * 1 for date in dates],\n",
    "            \"holiday\": np.random.choice([0, 1], n_rows, p=[0.97, 0.03]),\n",
    "            \"price_per_kg\": np.random.uniform(0.5, 3, n_rows),\n",
    "            \"month\": [date.month for date in dates],\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Introduce inflation over time (years)\n",
    "    df[\"inflation_multiplier\"] = 1 + (df[\"date\"].dt.year - df[\"date\"].dt.year.min()) * 0.03\n",
    "\n",
    "    # Incorporate seasonality due to apple harvests\n",
    "    df[\"harvest_effect\"] = np.sin(2 * np.pi * (df[\"month\"] - 3) / 12) + np.sin(\n",
    "        2 * np.pi * (df[\"month\"] - 9) / 12\n",
    "    )\n",
    "\n",
    "    # Modify the price_per_kg based on harvest effect\n",
    "    df[\"price_per_kg\"] = df[\"price_per_kg\"] - df[\"harvest_effect\"] * 0.5\n",
    "\n",
    "    # Adjust promo periods to coincide with periods lagging peak harvest by 1 month\n",
    "    peak_months = [4, 10]  # months following the peak availability\n",
    "    df[\"promo\"] = np.where(\n",
    "        df[\"month\"].isin(peak_months),\n",
    "        1,\n",
    "        np.random.choice([0, 1], n_rows, p=[0.85, 0.15]),\n",
    "    )\n",
    "\n",
    "    # Generate target variable based on features\n",
    "    base_price_effect = -df[\"price_per_kg\"] * 50\n",
    "    seasonality_effect = df[\"harvest_effect\"] * 50\n",
    "    promo_effect = df[\"promo\"] * 200\n",
    "\n",
    "    df[\"demand\"] = (\n",
    "        base_demand\n",
    "        + base_price_effect\n",
    "        + seasonality_effect\n",
    "        + promo_effect\n",
    "        + df[\"weekend\"] * 300\n",
    "        + np.random.normal(0, 50, n_rows)\n",
    "    ) * df[\"inflation_multiplier\"]  # adding random noise\n",
    "\n",
    "    # Add previous day's demand\n",
    "    df[\"previous_days_demand\"] = df[\"demand\"].shift(1)\n",
    "    df[\"previous_days_demand\"].fillna(method=\"bfill\", inplace=True)  # fill the first row\n",
    "\n",
    "    # Introduce competitor pricing\n",
    "    df[\"competitor_price_per_kg\"] = np.random.uniform(0.5, 3, n_rows)\n",
    "    df[\"competitor_price_effect\"] = (\n",
    "        df[\"competitor_price_per_kg\"] < df[\"price_per_kg\"]\n",
    "    ) * competitor_price_effect\n",
    "\n",
    "    # Stock availability based on past sales price (3 days lag with logarithmic decay)\n",
    "    log_decay = -np.log(df[\"price_per_kg\"].shift(3) + 1) + 2\n",
    "    df[\"stock_available\"] = np.clip(log_decay, 0.7, 1)\n",
    "\n",
    "    # Marketing intensity based on stock availability\n",
    "    # Identify where stock is above threshold\n",
    "    high_stock_indices = df[df[\"stock_available\"] > 0.95].index\n",
    "\n",
    "    # For each high stock day, increase marketing intensity for the next week\n",
    "    for idx in high_stock_indices:\n",
    "        df.loc[idx : min(idx + 7, n_rows - 1), \"marketing_intensity\"] = np.random.uniform(0.7, 1)\n",
    "\n",
    "    # If the marketing_intensity column already has values, this will preserve them;\n",
    "    #  if not, it sets default values\n",
    "    fill_values = pd.Series(np.random.uniform(0, 0.5, n_rows), index=df.index)\n",
    "    df[\"marketing_intensity\"].fillna(fill_values, inplace=True)\n",
    "\n",
    "    # Adjust demand with new factors\n",
    "    df[\"demand\"] = df[\"demand\"] + df[\"competitor_price_effect\"] + df[\"marketing_intensity\"]\n",
    "\n",
    "    # Drop temporary columns\n",
    "    df.drop(\n",
    "        columns=[\n",
    "            \"inflation_multiplier\",\n",
    "            \"harvest_effect\",\n",
    "            \"month\",\n",
    "            \"competitor_price_effect\",\n",
    "            \"stock_available\",\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>average_temperature</th>\n",
       "      <th>rainfall</th>\n",
       "      <th>weekend</th>\n",
       "      <th>holiday</th>\n",
       "      <th>price_per_kg</th>\n",
       "      <th>promo</th>\n",
       "      <th>demand</th>\n",
       "      <th>previous_days_demand</th>\n",
       "      <th>competitor_price_per_kg</th>\n",
       "      <th>marketing_intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-09-08 21:32:05.312976</td>\n",
       "      <td>30.584727</td>\n",
       "      <td>1.199291</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.726258</td>\n",
       "      <td>0</td>\n",
       "      <td>851.375336</td>\n",
       "      <td>851.276659</td>\n",
       "      <td>1.935346</td>\n",
       "      <td>0.098677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-09-09 21:32:05.312975</td>\n",
       "      <td>15.465069</td>\n",
       "      <td>1.037626</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.576471</td>\n",
       "      <td>0</td>\n",
       "      <td>906.855943</td>\n",
       "      <td>851.276659</td>\n",
       "      <td>2.344720</td>\n",
       "      <td>0.019318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-09-10 21:32:05.312975</td>\n",
       "      <td>10.786525</td>\n",
       "      <td>5.656089</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.513328</td>\n",
       "      <td>0</td>\n",
       "      <td>808.304909</td>\n",
       "      <td>906.836626</td>\n",
       "      <td>0.998803</td>\n",
       "      <td>0.409485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-09-11 21:32:05.312974</td>\n",
       "      <td>23.648154</td>\n",
       "      <td>12.030937</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.839225</td>\n",
       "      <td>0</td>\n",
       "      <td>1099.833810</td>\n",
       "      <td>857.895424</td>\n",
       "      <td>0.761740</td>\n",
       "      <td>0.872803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-09-12 21:32:05.312973</td>\n",
       "      <td>13.861391</td>\n",
       "      <td>4.303812</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.531772</td>\n",
       "      <td>0</td>\n",
       "      <td>1283.949061</td>\n",
       "      <td>1148.961007</td>\n",
       "      <td>2.123436</td>\n",
       "      <td>0.820779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>2024-05-12 21:32:05.309972</td>\n",
       "      <td>21.643051</td>\n",
       "      <td>3.821656</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2.391010</td>\n",
       "      <td>0</td>\n",
       "      <td>1591.882437</td>\n",
       "      <td>1596.799278</td>\n",
       "      <td>1.504432</td>\n",
       "      <td>0.756489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>2024-05-13 21:32:05.309971</td>\n",
       "      <td>13.808813</td>\n",
       "      <td>1.080603</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.898693</td>\n",
       "      <td>0</td>\n",
       "      <td>1312.870527</td>\n",
       "      <td>1641.125948</td>\n",
       "      <td>1.343586</td>\n",
       "      <td>0.742145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>2024-05-14 21:32:05.309970</td>\n",
       "      <td>11.698227</td>\n",
       "      <td>1.911000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.839860</td>\n",
       "      <td>0</td>\n",
       "      <td>987.065524</td>\n",
       "      <td>1312.128382</td>\n",
       "      <td>2.771896</td>\n",
       "      <td>0.742145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>2024-05-15 21:32:05.309969</td>\n",
       "      <td>18.052081</td>\n",
       "      <td>1.000521</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.188440</td>\n",
       "      <td>0</td>\n",
       "      <td>1397.886638</td>\n",
       "      <td>1036.323379</td>\n",
       "      <td>2.564075</td>\n",
       "      <td>0.742145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>2024-05-16 21:32:05.309964</td>\n",
       "      <td>17.017294</td>\n",
       "      <td>0.650213</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.131694</td>\n",
       "      <td>0</td>\n",
       "      <td>1289.584771</td>\n",
       "      <td>1397.144493</td>\n",
       "      <td>0.785727</td>\n",
       "      <td>0.833140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           date  average_temperature   rainfall  weekend  \\\n",
       "0    2010-09-08 21:32:05.312976            30.584727   1.199291        0   \n",
       "1    2010-09-09 21:32:05.312975            15.465069   1.037626        0   \n",
       "2    2010-09-10 21:32:05.312975            10.786525   5.656089        0   \n",
       "3    2010-09-11 21:32:05.312974            23.648154  12.030937        1   \n",
       "4    2010-09-12 21:32:05.312973            13.861391   4.303812        1   \n",
       "...                         ...                  ...        ...      ...   \n",
       "4995 2024-05-12 21:32:05.309972            21.643051   3.821656        1   \n",
       "4996 2024-05-13 21:32:05.309971            13.808813   1.080603        0   \n",
       "4997 2024-05-14 21:32:05.309970            11.698227   1.911000        0   \n",
       "4998 2024-05-15 21:32:05.309969            18.052081   1.000521        0   \n",
       "4999 2024-05-16 21:32:05.309964            17.017294   0.650213        0   \n",
       "\n",
       "      holiday  price_per_kg  promo       demand  previous_days_demand  \\\n",
       "0           0      1.726258      0   851.375336            851.276659   \n",
       "1           0      0.576471      0   906.855943            851.276659   \n",
       "2           0      2.513328      0   808.304909            906.836626   \n",
       "3           0      1.839225      0  1099.833810            857.895424   \n",
       "4           0      1.531772      0  1283.949061           1148.961007   \n",
       "...       ...           ...    ...          ...                   ...   \n",
       "4995        0      2.391010      0  1591.882437           1596.799278   \n",
       "4996        1      0.898693      0  1312.870527           1641.125948   \n",
       "4997        0      2.839860      0   987.065524           1312.128382   \n",
       "4998        0      1.188440      0  1397.886638           1036.323379   \n",
       "4999        0      2.131694      0  1289.584771           1397.144493   \n",
       "\n",
       "      competitor_price_per_kg  marketing_intensity  \n",
       "0                    1.935346             0.098677  \n",
       "1                    2.344720             0.019318  \n",
       "2                    0.998803             0.409485  \n",
       "3                    0.761740             0.872803  \n",
       "4                    2.123436             0.820779  \n",
       "...                       ...                  ...  \n",
       "4995                 1.504432             0.756489  \n",
       "4996                 1.343586             0.742145  \n",
       "4997                 2.771896             0.742145  \n",
       "4998                 2.564075             0.742145  \n",
       "4999                 0.785727             0.833140  \n",
       "\n",
       "[5000 rows x 11 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = generate_apple_sales_data_with_promo_adjustment(base_demand=1_000, n_rows=5000)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def plot_correlation_with_demand(df, save_path=None):\n",
    "    \"\"\"\n",
    "    Plots the correlation of each variable in the dataframe with the 'demand' column.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): DataFrame containing the data, including a 'demand' column.\n",
    "    - save_path (str, optional): Path to save the generated plot. If not specified, plot won't be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None (Displays the plot on a Jupyter window)\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute correlations between all variables and 'demand'\n",
    "    correlations = df.corr()[\"demand\"].drop(\"demand\").sort_values()\n",
    "\n",
    "    # Generate a color palette from red to green\n",
    "    colors = sns.diverging_palette(10, 130, as_cmap=True)\n",
    "    color_mapped = correlations.map(colors)\n",
    "\n",
    "    # Set Seaborn style\n",
    "    sns.set_style(\n",
    "        \"whitegrid\", {\"axes.facecolor\": \"#c2c4c2\", \"grid.linewidth\": 1.5}\n",
    "    )  # Light grey background and thicker grid lines\n",
    "\n",
    "    # Create bar plot\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    plt.barh(correlations.index, correlations.values, color=color_mapped)\n",
    "\n",
    "    # Set labels and title with increased font size\n",
    "    plt.title(\"Correlation with Demand\", fontsize=18)\n",
    "    plt.xlabel(\"Correlation Coefficient\", fontsize=16)\n",
    "    plt.ylabel(\"Variable\", fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.grid(axis=\"x\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if save_path is specified\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, format=\"png\", dpi=600)\n",
    "\n",
    "    # prevent matplotlib from displaying the chart every time we call this function\n",
    "    plt.close(fig)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Test the function\n",
    "correlation_plot = plot_correlation_with_demand(df, save_path=\"correlation_plot.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_residuals(model, dvalid, valid_y, save_path=None):\n",
    "    \"\"\"\n",
    "    Plots the residuals of the model predictions against the true values.\n",
    "\n",
    "    Args:\n",
    "    - model: The trained XGBoost model.\n",
    "    - dvalid (xgb.DMatrix): The validation data in XGBoost DMatrix format.\n",
    "    - valid_y (pd.Series): The true values for the validation set.\n",
    "    - save_path (str, optional): Path to save the generated plot. If not specified, plot won't be saved.\n",
    "\n",
    "    Returns:\n",
    "    - None (Displays the residuals plot on a Jupyter window)\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict using the model\n",
    "    preds = model.predict(dvalid)\n",
    "\n",
    "    # Calculate residuals\n",
    "    residuals = valid_y - preds\n",
    "\n",
    "    # Set Seaborn style\n",
    "    sns.set_style(\"whitegrid\", {\"axes.facecolor\": \"#c2c4c2\", \"grid.linewidth\": 1.5})\n",
    "\n",
    "    # Create scatter plot\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(valid_y, residuals, color=\"blue\", alpha=0.5)\n",
    "    plt.axhline(y=0, color=\"r\", linestyle=\"-\")\n",
    "\n",
    "    # Set labels, title and other plot properties\n",
    "    plt.title(\"Residuals vs True Values\", fontsize=18)\n",
    "    plt.xlabel(\"True Values\", fontsize=16)\n",
    "    plt.ylabel(\"Residuals\", fontsize=16)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=14)\n",
    "    plt.grid(axis=\"y\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the plot if save_path is specified\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, format=\"png\", dpi=600)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.close(fig)\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, booster):\n",
    "    \"\"\"\n",
    "    Plots feature importance for an XGBoost model.\n",
    "\n",
    "    Args:\n",
    "    - model: A trained XGBoost model\n",
    "\n",
    "    Returns:\n",
    "    - fig: The matplotlib figure object\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    importance_type = \"weight\" if booster == \"gblinear\" else \"gain\"\n",
    "    xgb.plot_importance(\n",
    "        model,\n",
    "        importance_type=importance_type,\n",
    "        ax=ax,\n",
    "        title=f\"Feature Importance based on {importance_type}\",\n",
    "    )\n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up the MLflow Experiment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start our hyperparameter tuning process, we need to designate a specific “experiment” within MLflow to track and log our results. An experiment in MLflow is essentially a named set of runs. Each run within an experiment tracks its own parameters, metrics, tags, and artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get_or_create_experiment function we’ve defined below aids in this process. It checks if an experiment with the specified name already exists. If yes, it retrieves its ID. If not, it creates a new experiment and returns its ID.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to try additional parameter ranges, different parameters, or a slightly modified dataset, we can use this Experiment to log all parent runs to keep our MLflow Tracking UI clean and easy to navigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment(experiment_name):\n",
    "    \"\"\"\n",
    "    Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.\n",
    "\n",
    "    This function checks if an experiment with the given name exists within MLflow.\n",
    "    If it does, the function returns its ID. If not, it creates a new experiment\n",
    "    with the provided name and returns its ID.\n",
    "\n",
    "    Parameters:\n",
    "    - experiment_name (str): Name of the MLflow experiment.\n",
    "\n",
    "    Returns:\n",
    "    - str: ID of the existing or newly created MLflow experiment.\n",
    "    \"\"\"\n",
    "\n",
    "    if experiment := mlflow.get_experiment_by_name(experiment_name):\n",
    "        return experiment.experiment_id\n",
    "    else:\n",
    "        return mlflow.create_experiment(experiment_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an experiment for our hyperparameter tuning runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = get_or_create_experiment(\"Apples Demand\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'853720947217136709'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up MLflow and Data Preprocessing for Model Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>We start by setting the MLflow experiment using the set_experiment function. The experiment_id serves as a unique identifier for the experiment, allowing us to segregate and manage different runs and their associated data efficiently</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the current active MLflow experiment\n",
    "mlflow.set_experiment(experiment_id=experiment_id)\n",
    "\n",
    "# Preprocess the dataset\n",
    "X = df.drop(columns=[\"date\", \"demand\"])\n",
    "y = df[\"demand\"]\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(X, y, test_size=0.25)\n",
    "dtrain = xgb.DMatrix(train_x, label=train_y)\n",
    "dvalid = xgb.DMatrix(valid_x, label=valid_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function serves as the core of our hyperparameter tuning process using Optuna. Additionally, it trains an XGBoost model using the selected hyperparameters and logs metrics and parameters to MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow Nested Runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function starts a new nested run in MLflow. Nested runs are useful for organizing hyperparameter tuning experiments as they allow you to group individual runs under a parent run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optuna’s trial.suggest_* methods are used to define a range of possible values for hyperparameters. Here’s what each hyperparameter does:\n",
    "\n",
    "*objective and eval_metric: Define the loss function and evaluation metric\n",
    "\n",
    "*booster: Type of boosting to be used (gbtree, gblinear, or dart)\n",
    "\n",
    "*lambda and alpha: Regularization parameters\n",
    "\n",
    "*Additional parameters like max_depth, eta, and gamma are specific to tree-based models (gbtree and dart)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logging with MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the selected hyperparameters and metrics (mse and rmse) are logged to MLflow for later analysis and comparison.\n",
    "\n",
    "*mlflow.log_params: Logs the hyperparameters.\n",
    "\n",
    "*mlflow.log_metric: Logs the metrics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Logging Callback:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The callback we’re about to introduce will modify the default reporting behavior. Instead of logging every trial, we’ll only receive updates when a new hyperparameter combination yields an improvement over the best metric value recorded thus far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the ensuing code, we:\n",
    "\n",
    "Adjust Optuna’s logging level to report only errors, ensuring a decluttered stdout.\n",
    "\n",
    "Define a champion_callback function, tailored to log only when a trial surpasses the previously recorded best metric.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override Optuna's default logging to ERROR only\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "# define a logging callback that will report on only new challenger parameter configurations if a\n",
    "# trial has usurped the state of 'best conditions'\n",
    "\n",
    "\n",
    "def champion_callback(study, frozen_trial):\n",
    "    \"\"\"\n",
    "    Logging callback that will report when a new trial iteration improves upon existing\n",
    "    best trial values.\n",
    "\n",
    "    Note: This callback is not intended for use in distributed computing systems such as Spark\n",
    "    or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n",
    "    workers or agents.\n",
    "    The race conditions with file system state management for distributed trials will render\n",
    "    inconsistent values with this callback.\n",
    "    \"\"\"\n",
    "\n",
    "    winner = study.user_attrs.get(\"winner\", None)\n",
    "\n",
    "    if study.best_value and winner != study.best_value:\n",
    "        study.set_user_attr(\"winner\", study.best_value)\n",
    "        if winner:\n",
    "            improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100\n",
    "            print(\n",
    "                f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n",
    "                f\"{improvement_percent: .4f}% improvement\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Define hyperparameters\n",
    "        params = {\n",
    "            \"objective\": \"reg:squarederror\",\n",
    "            \"eval_metric\": \"rmse\",\n",
    "            \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"gblinear\", \"dart\"]),\n",
    "            \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n",
    "            \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n",
    "        }\n",
    "\n",
    "        if params[\"booster\"] == \"gbtree\" or params[\"booster\"] == \"dart\":\n",
    "            params[\"max_depth\"] = trial.suggest_int(\"max_depth\", 1, 9)\n",
    "            params[\"eta\"] = trial.suggest_float(\"eta\", 1e-8, 1.0, log=True)\n",
    "            params[\"gamma\"] = trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True)\n",
    "            params[\"grow_policy\"] = trial.suggest_categorical(\n",
    "                \"grow_policy\", [\"depthwise\", \"lossguide\"]\n",
    "            )\n",
    "\n",
    "        # Train XGBoost model\n",
    "        bst = xgb.train(params, dtrain)\n",
    "        preds = bst.predict(dvalid)\n",
    "        error = mean_squared_error(valid_y, preds)\n",
    "\n",
    "        # Log to MLflow\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"mse\", error)\n",
    "        mlflow.log_metric(\"rmse\", math.sqrt(error))\n",
    "\n",
    "    return error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orchestrating Hyperparameter Tuning, Model Training, and Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"first_attempt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial trial 0 achieved value: 19181.54630434702\n",
      "Trial 3 achieved value: 14957.87640426004 with  28.2371% improvement\n",
      "Trial 48 achieved value: 14918.11325548162 with  0.2665% improvement\n",
      "Trial 52 achieved value: 14786.2108479878 with  0.8921% improvement\n",
      "Trial 110 achieved value: 14732.70712629793 with  0.3632% improvement\n",
      "Trial 134 achieved value: 14695.51664465138 with  0.2531% improvement\n",
      "Trial 152 achieved value: 14666.251547213771 with  0.1995% improvement\n",
      "Trial 169 achieved value: 14664.58477542703 with  0.0114% improvement\n"
     ]
    }
   ],
   "source": [
    "# Initiate the parent run and call the hyperparameter tuning child run logic\n",
    "with mlflow.start_run(experiment_id=experiment_id, run_name=run_name, nested=True):\n",
    "    # Initialize the Optuna study\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "\n",
    "    # Execute the hyperparameter optimization trials.\n",
    "    # Note the addition of the `champion_callback` inclusion to control our logging\n",
    "    study.optimize(objective, n_trials=500, callbacks=[champion_callback])\n",
    "\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metric(\"best_mse\", study.best_value)\n",
    "    mlflow.log_metric(\"best_rmse\", math.sqrt(study.best_value))\n",
    "\n",
    "    # Log tags\n",
    "    mlflow.set_tags(\n",
    "        tags={\n",
    "            \"project\": \"Apple Demand Project\",\n",
    "            \"optimizer_engine\": \"optuna\",\n",
    "            \"model_family\": \"xgboost\",\n",
    "            \"feature_set_version\": 1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Log a fit model instance\n",
    "    model = xgb.train(study.best_params, dtrain)\n",
    "\n",
    "    # Log the correlation plot\n",
    "    mlflow.log_figure(figure=correlation_plot, artifact_file=\"correlation_plot.png\")\n",
    "\n",
    "    # Log the feature importances plot\n",
    "    importances = plot_feature_importance(model, booster=study.best_params.get(\"booster\"))\n",
    "    mlflow.log_figure(figure=importances, artifact_file=\"feature_importances.png\")\n",
    "\n",
    "    # Log the residuals plot\n",
    "    residuals = plot_residuals(model, dvalid, valid_y)\n",
    "    mlflow.log_figure(figure=residuals, artifact_file=\"residuals.png\")\n",
    "\n",
    "    artifact_path = \"model\"\n",
    "\n",
    "    mlflow.xgboost.log_model(\n",
    "        xgb_model=model,\n",
    "        artifact_path=artifact_path,\n",
    "        input_example=train_x.iloc[[0]],\n",
    "        model_format=\"ubj\",\n",
    "        metadata={\"model_data_version\": 1},\n",
    "    )\n",
    "\n",
    "    # Get the logged model uri so that we can load it from the artifact store\n",
    "    model_uri = mlflow.get_artifact_uri(artifact_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mlflow-artifacts:/853720947217136709/349b248f9dda498589aadee48c011dfd/artifacts/model'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_uri"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Trained Model with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts:   0%|          | 0/10 [00:00<?, ?it/s]2024/05/16 21:33:02 INFO mlflow.store.artifact.artifact_repo: The progress bar can be disabled by setting the environment variable MLFLOW_ENABLE_ARTIFACTS_PROGRESS_BAR to false\n",
      "Downloading artifacts: 100%|██████████| 10/10 [00:00<00:00, 119.08it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loaded = mlflow.xgboost.load_model(model_uri)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
